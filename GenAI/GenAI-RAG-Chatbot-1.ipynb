{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1 style=\"color: #FF6347;\">Retrieval-Augmented Generation (RAGs)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**RAGs** (Retrieval-Augmented Generation) is an advanced application of Natural Language Processing (NLP) that combines document retrieval and generative models to provide context-aware, accurate, and dynamic responses. This technique is particularly useful for question-answering systems, chatbots, and technical document analysis.\n",
    "\n",
    "- **What is RAG?**\n",
    "  - Combines information retrieval with generative models.\n",
    "  - Retrieves relevant context from a document corpus or database and integrates it into generated responses.\n",
    "  - Designed for tasks requiring high accuracy and context sensitivity.\n",
    "\n",
    "- **Key Use Cases:**\n",
    "  - Question-answering systems.\n",
    "  - Chatbots that provide real-time, context-aware responses.\n",
    "  - Technical document analysis and summarization.\n",
    "  - Customer support with tailored, informed replies.\n",
    "\n",
    "- **Benefits of RAG:**\n",
    "  - Dynamically adapts to new information without retraining.\n",
    "  - Reduces hallucination in generative models.\n",
    "  - Enhances user interaction by grounding responses in verifiable data.\n",
    "\n",
    "\n",
    "<h3 style=\"color: #FF8C00;\">By the End of This Lesson, You'll:</h3>\n",
    "\n",
    "- Understand the fundamentals of Retrieval-Augmented Generation (RAGs).\n",
    "- Learn key text preprocessing techniques for RAGs.\n",
    "- Use word embeddings to create numerical representations of text.\n",
    "- Apply document retrieval techniques to find relevant context.\n",
    "- Employ generative models to create context-aware responses.\n",
    "- Analyze and interpret the generated responses for insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n genai python=3.10\n",
    "# pip install \"pydantic<2.0\"\n",
    "# pip install langchain==0.0.230\n",
    "# pip install python-dotenv\n",
    "# pip install openai\n",
    "# pip install --upgrade langchain pydantic\n",
    "# pip install -U langchain-community\n",
    "# pip install -U langchain-openai\n",
    "# pip install sentence-transformers\n",
    "# pip install pypdf\n",
    "# pip install chromadb\n",
    "\n",
    "# conda env export > environment.yml\n",
    "# conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
    "\n",
    "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
    "\n",
    "- **What Does PyPDFLoader Do?**\n",
    "  - Extracts text from PDF files, retaining formatting and layout.\n",
    "  - Simplifies the preprocessing of document-based datasets.\n",
    "  - Supports efficient and scalable loading of large PDF collections.\n",
    "\n",
    "- **Key Features:**\n",
    "  - Compatible with popular NLP libraries and frameworks.\n",
    "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
    "  - Provides flexible configurations for structured text extraction.\n",
    "\n",
    "- **Use Cases:**\n",
    "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
    "  - Automating the text extraction pipeline for document analysis.\n",
    "  - Creating datasets from academic papers, technical manuals, and reports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain_community\n",
    "# pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the document\n",
    "document_dir = \"./\"\n",
    "filename = \"The International System for Serous Fluid Cytopathology.pdf\"\n",
    "file_path = os.path.join(document_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
    "\n",
    "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks. \n",
    "\n",
    "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split the document\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
    "\n",
    "The `CharacterTextSplitter` utility helps divide text into smaller chunks, making it more manageable for downstream NLP tasks. This is particularly useful in workflows like Retrieval-Augmented Generation (RAG), where documents need to be processed as discrete sections.\n",
    "\n",
    "- **Code Explanation:**\n",
    "  - `CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)`:\n",
    "    - Initializes a text splitter with a specified chunk size and overlap.\n",
    "    - **`chunk_size=1000`**: Each chunk will contain up to 1,000 characters.\n",
    "    - **`chunk_overlap=0`**: No overlap between consecutive chunks.\n",
    "  - `split_documents(pages)`:\n",
    "    - Splits the input `pages` (e.g., from `PyPDFLoader`) into smaller text chunks.\n",
    "  - `chunks`: The resulting list of chunks, each containing a portion of the original document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split pages into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
    "\n",
    "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
    "\n",
    "- **What are OpenAI Embeddings?**\n",
    "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
    "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
    "\n",
    "- **Key Features of `text-embedding-3-large`:**\n",
    "  - Large-scale embedding model optimized for accuracy and versatility.\n",
    "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
    "  - Ideal for applications with high-performance requirements.\n",
    "\n",
    "- **Benefits:**\n",
    "  - Reduces the need for extensive custom training.\n",
    "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
    "  - Compatible with RAGs to create powerful context-aware models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminal: \n",
    "# echo OPENAI_API_KEY=\"\" > .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/dp25t9yj1t7fql93jpgg11jm0000gn/T/ipykernel_8884/2847390941.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
    "\n",
    "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
    "\n",
    "### Workflow Overview:\n",
    "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
    "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
    "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
    "\n",
    "### Key Features of ChromaDB:\n",
    "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
    "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
    "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma\n",
    "# pip install --upgrade pip setuptools wheel\n",
    "# pip install duckdb --only-binary=:all:\n",
    "# pip install chroma-migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB created with document embeddings.\n"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
    "print(\"ChromaDB created with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How can I diagnose a malignant mesothelioma?\" # User question\n",
    "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "The definitive diagnosis of malignant mesotheli-\n",
      "oma can still be made with the use of ancillary studies (IC, analysis of soluble bio-\n",
      "markers, and p16 FISH). Some patients have thoracoscopic biopsies of parietal \n",
      "pleura which are diagnosed as MIS. However, treatment of patients with MIS is not \n",
      "yet established. According to the NCCN guidelines, malignant mesothelioma can \n",
      "be treated with chemotherapy only if inoperable. Operable mesothelioma patients \n",
      "may be treated with pleurectomy/decortication or extrapleural pneumonectomy \n",
      "(EPP). Patients with operable mesothelioma can receive chemotherapy either before \n",
      "or after surgery. In patients who do not receive induction chemotherapy before EPP, \n",
      "postoperative sequential chemotherapy with hemithoracic radiation therapy is rec-\n",
      "ommended [86]. The detection of malignant mesothelioma early enough to improve \n",
      "chances for curative treatment would probably require screening blood samples and \n",
      "the employment of\n"
     ]
    }
   ],
   "source": [
    "# Display top results\n",
    "for i, doc in enumerate(retrieved_docs[:1]): # Display top 3 results\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_document_prompt(docs):\n",
    "    prompt = \"\\n\"\n",
    "    for doc in docs:\n",
    "        prompt += \"\\nContent:\\n\"\n",
    "        prompt += doc.page_content + \"\\n\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context formatted for GPT model.\n"
     ]
    }
   ],
   "source": [
    "# Generate a formatted context from the retrieved documents\n",
    "formatted_context = _get_document_prompt(retrieved_docs)\n",
    "print(\"Context formatted for GPT model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt constructed.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "## SYSTEM ROLE\n",
    "You are a knowledgeable and factual chatbot designed to assist with technical questions about **Cytology**, specifically focusing on **Lung Cancer**. \n",
    "Your answers must be based exclusively on provided content from technical books provided.\n",
    "\n",
    "## USER QUESTION\n",
    "The user has asked: \n",
    "\"{user_question}\"\n",
    "\n",
    "## CONTEXT\n",
    "Here is the relevant content from the technical books:  \n",
    "'''\n",
    "{formatted_context}\n",
    "'''\n",
    "\n",
    "## GUIDELINES\n",
    "1. **Accuracy**:  \n",
    "   - Only use the content in the `CONTEXT` section to answer.  \n",
    "   - If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
    "   - Start explain cell morphology and then divide morphology in bulletpoints (nuclie, cytoplasm, background and other aspects to consider) \n",
    "   - Follow by differential diagnosis\n",
    "   - Lastly explain ancillary studies for malignant mesothelioma.\n",
    "\n",
    "2. **Transparency**:  \n",
    "   - Reference the book's name and page numbers when providing information.  \n",
    "   - Do not speculate or provide opinions.  \n",
    "\n",
    "3. **Clarity**:  \n",
    "   - Use simple, professional, and concise language.  \n",
    "   - Format your response in Markdown for readability.  \n",
    "\n",
    "## TASK\n",
    "1. Answer the user's question **directly** if possible.  \n",
    "2. Point the user to relevant parts of the documentation.  \n",
    "3. Provide the response in the following format:\n",
    "\n",
    "## RESPONSE FORMAT\n",
    "'''\n",
    "# [Brief Title of the Answer]\n",
    "[Answer in simple, clear text.]\n",
    "\n",
    "**Source**:  \n",
    "• [Book Title], Page(s): [...]\n",
    "'''\n",
    "\"\"\"\n",
    "print(\"Prompt constructed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPT client and parameters\n",
    "client = openai.OpenAI()\n",
    "model_params = {\n",
    "    'model': 'gpt-4o',\n",
    "    'temperature': 0.7,  # Increase creativity\n",
    "    'max_tokens': 4000,  # Allow for longer responses\n",
    "    'top_p': 0.9,        # Use nucleus sampling\n",
    "    'frequency_penalty': 0.5,  # Reduce repetition\n",
    "    'presence_penalty': 0.6    # Encourage new topics\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF6347;\">Response</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role': 'user', 'content': prompt}]\n",
    "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\n",
      "# Diagnosing Malignant Mesothelioma\n",
      "\n",
      "To diagnose malignant mesothelioma, a combination of cytological examination and ancillary studies is required. Here is a detailed breakdown:\n",
      "\n",
      "### Cell Morphology\n",
      "The definitive diagnosis relies heavily on the morphological characteristics of cells in body cavity fluids.\n",
      "\n",
      "- **Nuclei**: Malignant cells typically present with prominent nucleoli and an increased nuclear-to-cytoplasmic ratio.\n",
      "- **Cytoplasm**: The cytoplasm may appear vacuolated or dense, indicative of malignancy.\n",
      "- **Background**: A bloody effusion might be present, which is common in cases of malignant mesothelioma.\n",
      "- **Other Aspects to Consider**: Cells are positive for mesothelial markers and negative for epithelial markers in immunochemistry tests.\n",
      "\n",
      "### Differential Diagnosis\n",
      "The differential diagnosis includes distinguishing malignant mesothelioma from other mesothelial tumors or conditions such as:\n",
      "\n",
      "- Mesothelioma in situ (MIS)\n",
      "- Other epithelial malignancies\n",
      "\n",
      "### Ancillary Studies\n",
      "Ancillary studies play a crucial role in confirming the diagnosis:\n",
      "\n",
      "- **Immunochemistry (IC)**: Supports the diagnosis by identifying specific markers. \n",
      "- **Soluble Biomarkers**: Increased levels can corroborate the presence of malignancy.\n",
      "- **Loss of BAP1 Expression**: Indicates malignancy when observed.\n",
      "- **p16 FISH Test**: Deletion of p16/CDKN2 supports the diagnosis of malignant mesothelioma.\n",
      "\n",
      "A thoracoscopic biopsy may be suggested to exclude other possibilities like MIS if necessary.\n",
      "\n",
      "**Source**:  \n",
      "• C. Michael et al., Page(s): 92, 64\n",
      "'''\n"
     ]
    }
   ],
   "source": [
    "answer = completion.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6f7ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<h2>Diagnosing Malignant Mesothelioma</h2>\n",
    "\n",
    "To diagnose malignant mesothelioma, a combination of cytological examination, differential diagnosis, and ancillary studies is utilized.\n",
    "\n",
    "### Cell Morphology\n",
    "- **Nuclei**: The cells may present with pyknotic nuclei, which are condensed and darkly stained.\n",
    "- **Cytoplasm**: Cells often appear eosinophilic, meaning they have a pinkish hue when stained with Papanicolaou stain.\n",
    "- **Cell Clusters**: Mesothelioma cells may form three-dimensional clusters such as papillary or acinar formations.\n",
    "\n",
    "### Differential Diagnosis\n",
    "- **Malignant Mesothelioma (MAL-P)**: Diagnosed when overtly malignant features are identified, often confirmed by ancillary tests.\n",
    "- **Suspicious for Malignancy (SFM)**: Used when features suggest malignancy but are not confirmed by ancillary tests.\n",
    "- **Atypia of Undetermined Significance (AUS)**: Used sparingly when the clinical picture does not support a reactive etiology and ancillary tests are inconclusive.\n",
    "\n",
    "### Ancillary Studies\n",
    "- **Immunocytochemistry (IC)**: Used to identify mesothelial markers and exclude epithelial markers.\n",
    "- **Fluorescence In Situ Hybridization (FISH)**: Detects deletion of p16/CDKN2A, which supports the diagnosis.\n",
    "- **Biomarker Analysis**: Soluble biomarkers like hyaluronan, mesothelin, and osteopontin can aid in diagnosis.\n",
    "- **Loss of BAP1 Expression**: This is another supportive test for diagnosing malignant mesothelioma.\n",
    "\n",
    "**Source**:  \n",
    "• C. Michael et al., Pages: 64, 74, 90, 92, 98\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF6347;\">OpenAI Vs SentenceTransformer Embeddings</h2>\n",
    "\n",
    "Embeddings are critical for transforming text into dense vector representations. Comparing different embedding models helps us:\n",
    "\n",
    "1. Understand their performance in tasks like similarity search and context retrieval.\n",
    "2. Determine which model is better suited for specific applications:\n",
    "   - **OpenAI Embeddings**: High-quality, general-purpose embeddings for robust tasks.\n",
    "   - **SentenceTransformers**: Lightweight, domain-specific embeddings, optimized for speed and efficiency.\n",
    "\n",
    "In this section, we'll compare:\n",
    "- Vector dimensions.\n",
    "- Example embedding outputs for the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-huggingface\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7z/dp25t9yj1t7fql93jpgg11jm0000gn/T/ipykernel_8884/2332591894.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_vector = openai_embeddings.embed_query(user_question)\n",
    "sentence_vector = sentence_transformer_embeddings.embed_query(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openai_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the higher-dimensional embedding to match the lower-dimensional one\n",
    "if len(openai_vector) > len(sentence_vector):\n",
    "\topenai_vector = openai_vector[:len(sentence_vector)] # slice the vector to match the length of the other\n",
    "elif len(sentence_vector) > len(openai_vector):\n",
    "\tsentence_vector = sentence_vector[:len(openai_vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openai_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common elements: set()\n",
      "\n",
      "1. OpenAI Embeddings: \n",
      "   {0.018350934609770775, 0.006769313011318445, 0.0035643160808831453, 0.018132776021957397, -0.01743980497121811}\n",
      "\n",
      "2. SentenceTransformer Embeddings: \n",
      "    {0.008178613148629665, 0.08663614839315414, -0.021265855059027672, 0.0016131369629874825, -0.007358015514910221}\n"
     ]
    }
   ],
   "source": [
    "# Compare the two sets\n",
    "common_elements = set(openai_vector[:5]).intersection(set(sentence_vector[:5]))\n",
    "unique_to_openai = set(openai_vector[:5]) - set(sentence_vector[:5])\n",
    "unique_to_sentence_transformer = set(sentence_vector[:5]) - set(openai_vector[:5])\n",
    "\n",
    "print(f\"Common elements: {common_elements}\\n\")\n",
    "print(f\"1. OpenAI Embeddings: \\n   {unique_to_openai}\\n\")\n",
    "print(f\"2. SentenceTransformer Embeddings: \\n    {unique_to_sentence_transformer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between OpenAI and SentenceTransformer embeddings: 0.0342\n"
     ]
    }
   ],
   "source": [
    "similarity = 1 - cosine(openai_vector, sentence_vector)\n",
    "print(f\"Cosine Similarity between OpenAI and SentenceTransformer embeddings: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
    "\n",
    "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It provides a scale from -1 to 1:\n",
    "\n",
    "- **-1**: Vectors are completely opposite.\n",
    "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
    "- **1**: Vectors are identical.\n",
    "\n",
    "<h3 style=\"color: #FF8C00;\">OpenAI Vs SentenceTransformer</h3>\n",
    "\n",
    "A **cosine similarity score of 0.0342** suggests that the embeddings from OpenAI and SentenceTransformer are almost orthogonal, meaning they capture **different aspects of the text**. This result highlights:\n",
    "\n",
    "- **Model Architecture Differences**: Each model is trained using distinct methodologies and objectives.\n",
    "- **Diverse Training Data**: The models may have been exposed to varying datasets, leading to differences in how they represent semantic relationships.\n",
    "- **Embedding Techniques**: Differences in how text is tokenized and transformed into vectors can lead to orthogonality in embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
    "\n",
    "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query.\n",
    "\n",
    "Using the query keywords `[\"malignant\", \"mesothelioma\", \"diagnosis\"]`, this code snippet identifies and highlights their occurrences in the retrieved text. Here's how it works:\n",
    "\n",
    "- **Process**:\n",
    "  - Iterate through the top 1 retrieved document.\n",
    "  - Extract the first 200 characters from each document.\n",
    "  - Highlight the keywords using the `highlight_keywords` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_keywords(text, keywords):\n",
    "    for keyword in keywords:\n",
    "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet 1:\n",
      "92\n",
      "of typical pleural \u001b[1m\u001b[32mmesothelioma\u001b[0m. The definitive \u001b[1m\u001b[32mdiagnosis\u001b[0m of \u001b[1m\u001b[32mmalignant\u001b[0m mesotheli-\n",
      "oma can still be made with the use of ancillary studies (IC, analysis of soluble bio-\n",
      "markers, and p16 FISH). Some \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query_keywords = [\"malignant\", \"mesothelioma\", \"diagnosis\"]\n",
    "for i, doc in enumerate(retrieved_docs[:1]):\n",
    "    snippet = doc.page_content[:200]\n",
    "    highlighted = highlight_keywords(snippet, query_keywords)\n",
    "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `query_keywords` is a list of keywords to be highlighted.\n",
    "2. The loop iterates over the first three documents in retrieved_docs.\n",
    "3. For each document, a snippet of the first 200 characters is extracted.\n",
    "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
    "5. The highlighted snippet is printed along with a separator line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF6347;\">Summary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Document Loading and Preprocessing**:\n",
    "   - Used `PyPDFLoader` to load and split PDFs into manageable chunks.\n",
    "   - Preprocessed text into embeddings for efficient similarity search.\n",
    "\n",
    "2. **Embedding Creation**:\n",
    "   - Generated embeddings using OpenAI and SentenceTransformer models.\n",
    "\n",
    "3. **Data Storage**:\n",
    "   - Stored embeddings in ChromaDB for fast and scalable retrieval.\n",
    "\n",
    "4. **Document Retrieval**:\n",
    "   - Queried ChromaDB to retrieve relevant snippets based on user input.\n",
    "\n",
    "5. **Answer Generation**:\n",
    "   - Formatted retrieved content into a structured prompt for generative AI.\n",
    "   - Produced context-aware responses using GPT models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
